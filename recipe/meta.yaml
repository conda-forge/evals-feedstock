{% set name = "evals" %}
{% set version = "1.0.3.post1" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://github.com/openai/evals/archive/refs/tags/{{ version }}.tar.gz
  sha256: d24f525e0051d48662f38f4f1216d61ea0079df5a5ec0ee5582c28b7eeed4661

build:
  entry_points:
    - oaieval = evals.cli.oaieval:main
    - oaievalset = evals.cli.oaievalset:main
  noarch: python
  script: {{ PYTHON }} -m pip install . -vv
  number: 1

requirements:
  host:
    - python {{ python_min }}
    - pip
  run:
    - python >={{ python_min }}
    - mypy
    - openai >=0.27.2
    - tiktoken
    - blobfile
    - backoff
    - numpy
    - snowflake-connector-python
    - pandas
    - fire
    - pydantic
    - tqdm
    - nltk
    - filelock
    - mock
    - langdetect
    - termcolor
    - lz4
    - pyzstd
    - pyyaml
    - sacrebleu
    - matplotlib-base
    - setuptools-scm

test:
  imports:
    - evals
  commands:
    - pip check
    - oaieval --help
    - oaievalset --help
  requires:
    - pip
    - python {{ python_min }}

about:
  home: https://github.com/openai/evals
  summary: Evals is a framework for evaluating OpenAI models and an open-source registry of benchmarks.
  license: MIT
  license_file: LICENSE

extra:
  recipe-maintainers:
    - BastianZim
