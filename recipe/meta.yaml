{% set name = "evals" %}
{% set version = "1.0.2.post1" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/evals-{{ version }}.tar.gz
  sha256: af95733163093d5a3bb5dfab557447638eac06532e83c31dd177418c91835c6d

build:
  entry_points:
    - oaieval = evals.cli.oaieval:main
    - oaievalset = evals.cli.oaievalset:main
  noarch: python
  script: {{ PYTHON }} -m pip install . -vv
  number: 0

requirements:
  host:
    - python >=3.8
    - pip
  run:
    - python >=3.8
    - mypy
    - openai
    - tiktoken
    - blobfile
    - backoff
    - numpy
    - snowflake-connector-python
    - pandas
    - fire
    - pydantic
    - tqdm
    - nltk
    - filelock
    - mock
    - langdetect
    - termcolor
    - lz4
    - pyzstd
    - pyyaml
    - sacrebleu
    - matplotlib-base

test:
  imports:
    - evals
  commands:
    - pip check
    - oaieval --help
    - oaievalset --help
  requires:
    - pip

about:
  home: https://github.com/openai/evals
  summary: Evals is a framework for evaluating OpenAI models and an open-source registry of benchmarks.
  license: MIT
  license_file: LICENSE

extra:
  recipe-maintainers:
    - BastianZim
