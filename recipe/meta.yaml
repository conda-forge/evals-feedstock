{% set name = "evals" %}
{% set version = "3.0.1.post1" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/evals-{{ version }}.tar.gz
  sha256: 0ffd9bed75c273a4a9f0b10ccf11a5e1e1c63c89926f12958375551429a4dc21

build:
  entry_points:
    - oaieval = evals.cli.oaieval:main
    - oaievalset = evals.cli.oaievalset:main
  noarch: python
  script: {{ PYTHON }} -m pip install . -vv
  number: 0

requirements:
  host:
    - python >=3.9
    - pip
  run:
    - aiolimiter
    - anthropic
    - beartype >=0.12.0
    - chess
    - dacite
    - datasets
    - docker-py
    - evaluate
    - flask
    - google-generativeai
    - gymnasium
    - jiwer
    - langchain
    - networkx
    - numexpr
    - playwright
    - pytest
    - seaborn
    - spacy-universal-sentence-encoder
    - statsmodels
    - types-pyyaml
    - types-tqdm
    - zstandard
    - python >=3.9
    - mypy
    - openai >=1.0.0
    - tiktoken
    - blobfile
    - backoff
    - numpy
    - snowflake-connector-python
    - pandas
    - fire
    - pydantic
    - tqdm
    - nltk
    - filelock
    - mock
    - langdetect
    - termcolor
    - lz4
    - pyzstd
    - pyyaml
    - sacrebleu
    - matplotlib-base
    - setuptools-scm

test:
  imports:
    - evals
  commands:
    - pip check
    - oaieval --help
    - oaievalset --help
  requires:
    - pip

about:
  home: https://github.com/openai/evals
  summary: Evals is a framework for evaluating OpenAI models and an open-source registry of benchmarks.
  license: MIT
  license_file: LICENSE

extra:
  recipe-maintainers:
    - BastianZim
